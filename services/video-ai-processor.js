// Video + Voice AI Processing Service for ListPro
// Combines speech-to-text with computer vision for rich listing generation

const FormData = require('form-data');
const fs = require('fs');

class VideoAIProcessor {
  constructor() {
    this.openaiApiKey = process.env.OPENAI_API_KEY;
    this.supabase = require('./supabase-client').supabase;
  }

  // Main processing pipeline
  async processVideoListing(videoUri, userId) {
    try {
      console.log('üé¨ Starting video processing pipeline...');
      
      // Step 1: Extract audio and get transcript
      const audioData = await this.extractAudioFromVideo(videoUri);
      const transcript = await this.speechToText(audioData);
      
      // Step 2: Extract key frames for visual analysis
      const frames = await this.extractKeyFrames(videoUri, 5);
      const visualAnalysis = await this.analyzeVideoFrames(frames);
      
      // Step 3: Combine voice + visual with multimodal AI
      const combinedAnalysis = await this.multimodalAnalysis(transcript, visualAnalysis);
      
      // Step 4: Generate platform-specific content
      const platformContent = await this.generatePlatformContent(combinedAnalysis);
      
      // Step 5: Store results
      const listing = await this.createListingFromAnalysis(combinedAnalysis, platformContent, userId);
      
      return {
        success: true,
        listingId: listing.id,
        analysis: combinedAnalysis,
        platformContent: platformContent,
        confidence: combinedAnalysis.confidence
      };
      
    } catch (error) {
      console.error('Error processing video:', error);
      throw new Error(`Video processing failed: ${error.message}`);
    }
  }

  // Extract audio from video file
  async extractAudioFromVideo(videoUri) {
    // In production, use FFmpeg or similar
    // For MVP, assume video already has audio track
    return {
      audioUri: videoUri, // Simplified for demo
      duration: 30, // seconds
      format: 'mp4'
    };
  }

  // Convert speech to text using OpenAI Whisper
  async speechToText(audioData) {
    console.log('üé§ Converting speech to text...');
    
    const formData = new FormData();
    formData.append('file', fs.createReadStream(audioData.audioUri));
    formData.append('model', 'whisper-1');
    formData.append('language', 'en');
    formData.append('response_format', 'verbose_json');

    const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        ...formData.getHeaders()
      },
      body: formData
    });

    if (!response.ok) {
      throw new Error(`Whisper API error: ${response.status}`);
    }

    const result = await response.json();
    
    return {
      fullText: result.text,
      segments: result.segments, // Time-stamped segments
      language: result.language,
      confidence: this.calculateAverageConfidence(result.segments),
      duration: result.duration,
      words: result.words || []
    };
  }

  // Extract key frames from video
  async extractKeyFrames(videoUri, count = 5) {
    console.log('üñºÔ∏è Extracting key frames...');
    
    // In production, use FFmpeg to extract frames at intervals
    // For demo, simulate frame extraction
    const frames = [];
    
    for (let i = 0; i < count; i++) {
      frames.push({
        timestamp: (i / (count - 1)) * 30, // Distribute across 30 seconds
        frameUri: `${videoUri}_frame_${i}.jpg`,
        index: i
      });
    }
    
    return frames;
  }

  // Analyze video frames with computer vision
  async analyzeVideoFrames(frames) {
    console.log('üëÅÔ∏è Analyzing video frames...');
    
    const frameAnalyses = [];
    
    for (const frame of frames) {
      const analysis = await this.analyzeFrame(frame);
      frameAnalyses.push(analysis);
    }
    
    return {
      frames: frameAnalyses,
      detectedObjects: this.consolidateObjects(frameAnalyses),
      dominantColors: this.extractDominantColors(frameAnalyses),
      sceneContext: this.analyzeSceneContext(frameAnalyses)
    };
  }

  // Analyze single frame with GPT-4 Vision
  async analyzeFrame(frame) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4-vision-preview',
        messages: [{
          role: 'user',
          content: [
            {
              type: 'text',
              text: 'Analyze this image for items being sold. Identify: brand, model, condition, color, material, size, and any distinguishing features. Be specific and detailed.'
            },
            {
              type: 'image_url',
              image_url: { url: frame.frameUri }
            }
          ]
        }],
        max_tokens: 500
      })
    });

    const result = await response.json();
    
    return {
      timestamp: frame.timestamp,
      description: result.choices[0].message.content,
      detectedItems: this.extractItems(result.choices[0].message.content),
      confidence: 0.85 // Simulated confidence score
    };
  }

  // Combine voice transcript with visual analysis
  async multimodalAnalysis(transcript, visualAnalysis) {
    console.log('üß† Performing multimodal analysis...');
    
    const prompt = `
    You are an expert at analyzing items for sale. Combine the visual and audio information to create a comprehensive item analysis.

    VISUAL ANALYSIS:
    ${visualAnalysis.frames.map(f => f.description).join('\n')}

    VOICE TRANSCRIPT:
    "${transcript.fullText}"

    Extract and organize:

    1. ITEM DETAILS:
       - Brand, model, year/vintage
       - Category and subcategory
       - Condition (new, like-new, good, fair, poor)
       - Size, color, material
       - Key features and specifications

    2. EMOTIONAL CONTEXT:
       - Seller's attachment level (high/medium/low)
       - Reason for selling
       - Enthusiasm level
       - Any personal stories or history

    3. SELLING POINTS:
       - Unique features mentioned
       - Condition details from voice
       - Provenance or authenticity indicators
       - Value propositions

    4. PRICING SIGNALS:
       - Original purchase price mentioned
       - Condition assessment
       - Rarity or collectible value
       - Urgency to sell

    5. OPTIMIZATION RECOMMENDATIONS:
       - Best marketplace platforms for this item
       - Suggested listing strategy
       - Key selling points to emphasize
       - Potential buyer personas

    Return a structured JSON response with high confidence scores for accurate extractions.
    `;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4-turbo-preview',
        messages: [{ role: 'user', content: prompt }],
        response_format: { type: 'json_object' },
        temperature: 0.3
      })
    });

    const result = await response.json();
    const analysis = JSON.parse(result.choices[0].message.content);

    return {
      ...analysis,
      transcript: transcript,
      visualData: visualAnalysis,
      processingTimestamp: new Date().toISOString(),
      confidence: this.calculateOverallConfidence(transcript, visualAnalysis, analysis)
    };
  }

  // Generate platform-specific content
  async generatePlatformContent(analysis) {
    console.log('üìù Generating platform-specific content...');
    
    const platforms = ['ebay', 'etsy', 'poshmark', 'instagram', 'facebook'];
    const content = {};

    for (const platform of platforms) {
      content[platform] = await this.generateContentForPlatform(analysis, platform);
    }

    return content;
  }

  // Generate content for specific platform
  async generateContentForPlatform(analysis, platform) {
    const platformSpecs = {
      ebay: {
        titleLimit: 80,
        descriptionLimit: 500000,
        style: 'factual and detailed',
        keyFeatures: ['brand', 'model', 'condition', 'specifications']
      },
      poshmark: {
        titleLimit: 50,
        descriptionLimit: 8000,
        style: 'casual and personal',
        keyFeatures: ['brand', 'style', 'story', 'hashtags']
      },
      instagram: {
        captionLimit: 2200,
        style: 'engaging and visual',
        keyFeatures: ['lifestyle', 'aesthetics', 'hashtags', 'call-to-action']
      },
      etsy: {
        titleLimit: 140,
        descriptionLimit: 13000,
        style: 'artisanal and story-driven',
        keyFeatures: ['craftsmanship', 'vintage', 'unique features']
      },
      facebook: {
        titleLimit: 100,
        descriptionLimit: 9000,
        style: 'community-friendly',
        keyFeatures: ['local appeal', 'value proposition', 'condition']
      }
    };

    const spec = platformSpecs[platform];
    
    const prompt = `
    Create an optimized listing for ${platform.toUpperCase()} based on this item analysis:

    ${JSON.stringify(analysis.itemDetails, null, 2)}

    Platform requirements:
    - Title limit: ${spec.titleLimit} characters
    - Description limit: ${spec.descriptionLimit} characters  
    - Style: ${spec.style}
    - Focus on: ${spec.keyFeatures.join(', ')}

    ${platform === 'poshmark' || platform === 'instagram' ? 'Include relevant hashtags.' : ''}
    ${platform === 'instagram' ? 'Include call-to-action and engagement hooks.' : ''}

    Generate:
    1. Optimized title
    2. Compelling description
    3. Key selling points
    ${platform === 'poshmark' || platform === 'instagram' ? '4. Relevant hashtags' : ''}
    ${platform === 'instagram' ? '5. Suggested caption hooks' : ''}

    Make it authentic and true to the seller's voice from the transcript.
    `;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${this.openaiApiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'gpt-4-turbo-preview',
        messages: [{ role: 'user', content: prompt }],
        temperature: 0.7
      })
    });

    const result = await response.json();
    
    return {
      platform: platform,
      content: result.choices[0].message.content,
      generatedAt: new Date().toISOString(),
      optimizationScore: this.calculateOptimizationScore(analysis, platform)
    };
  }

  // Create listing in database
  async createListingFromAnalysis(analysis, platformContent, userId) {
    const listing = {
      user_id: userId,
      title: analysis.itemDetails?.title || 'Untitled Item',
      description: analysis.itemDetails?.description || '',
      price: analysis.pricingSignals?.suggestedPrice || 0,
      category: analysis.itemDetails?.category || 'other',
      condition: analysis.itemDetails?.condition || 'good',
      brand: analysis.itemDetails?.brand,
      ai_metadata: {
        confidence: analysis.confidence,
        transcript: analysis.transcript,
        visualAnalysis: analysis.visualData,
        emotionalContext: analysis.emotionalContext,
        sellingPoints: analysis.sellingPoints,
        platformContent: platformContent
      },
      ai_suggested_price: analysis.pricingSignals?.suggestedPrice,
      ai_confidence_score: analysis.confidence,
      ai_processing_status: 'completed'
    };

    const { data, error } = await this.supabase
      .from('listings')
      .insert(listing)
      .select()
      .single();

    if (error) throw error;
    return data;
  }

  // Helper methods
  calculateAverageConfidence(segments) {
    if (!segments?.length) return 0.8;
    const total = segments.reduce((sum, seg) => sum + (seg.confidence || 0.8), 0);
    return total / segments.length;
  }

  consolidateObjects(frameAnalyses) {
    // Combine detected objects across frames
    const objects = {};
    frameAnalyses.forEach(frame => {
      frame.detectedItems?.forEach(item => {
        objects[item.name] = (objects[item.name] || 0) + 1;
      });
    });
    return objects;
  }

  extractDominantColors(frameAnalyses) {
    // Extract and consolidate color information
    return ['blue', 'silver', 'black']; // Simplified
  }

  analyzeSceneContext(frameAnalyses) {
    // Determine setting, lighting, etc.
    return {
      setting: 'indoor',
      lighting: 'natural',
      background: 'neutral'
    };
  }

  extractItems(description) {
    // Parse description for structured item data
    return []; // Simplified
  }

  calculateOverallConfidence(transcript, visualAnalysis, analysis) {
    // Combine confidence scores from different sources
    const transcriptConf = transcript.confidence || 0.8;
    const visualConf = visualAnalysis.frames?.reduce((avg, f) => avg + f.confidence, 0) / (visualAnalysis.frames?.length || 1) || 0.8;
    const analysisConf = 0.9; // High confidence in GPT-4 analysis
    
    return (transcriptConf + visualConf + analysisConf) / 3;
  }

  calculateOptimizationScore(analysis, platform) {
    // Score how well content is optimized for platform
    return Math.random() * 0.3 + 0.7; // 0.7-1.0 range
  }
}

module.exports = VideoAIProcessor;